{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48d35522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import json\n",
    "# import xlsxwriter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "452a8248",
   "metadata": {},
   "source": [
    "### Non Parametric Tests - Ordinal/ Nominal  DVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49639559",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'RawData.xlsx'\n",
    "sheet_name = 'Survey Raw Data'\n",
    "data_workbook = \"SampleData.xlsx\"\n",
    "significant_val = 0.05\n",
    "len_df = 499 # this should be infered from the data rather being assigned\n",
    "\n",
    "# encoding headers ( not necessary unless there is a heirarchy of headers)\n",
    "df = pd.read_excel(file_name, sheet_name = sheet_name)\n",
    "df_heading = code_encode_headers(df)\n",
    "\n",
    "# variable dictionary that stores call coded headers ( not necessary. only used as my data had heirarchy of headers)\n",
    "with open('CodeJsonVar', 'r') as file:\n",
    "    dic = json.load(file)\n",
    "\n",
    "# storing resukts in a data frame\n",
    "df_result = ONStatisticTests(data_workbook, dic, df_heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "208d5b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_excel('SampleResults.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9e6648e",
   "metadata": {},
   "source": [
    "### Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff6ec7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_encode_headers(df):\n",
    "\n",
    "\n",
    "    # gather all header and sub-header names from the three rows\n",
    "    header_first =  [pd.NA if \"Unnamed\" in x else x for x in list(df.columns)]\n",
    "    header_second = list(df.iloc[0])\n",
    "    header_third = list(df.iloc[1])\n",
    "\n",
    "    # create df from for the headings\n",
    "    heading = {'header_first': header_first, 'header_second': header_second, 'header_third' : header_third}\n",
    "    df_heading = pd.DataFrame(heading)\n",
    "    df_heading[['header_first', 'header_second']] = df_heading[['header_first', 'header_second']].fillna(method = 'ffill')\n",
    "\n",
    "    num = (df_heading['header_first'] == 'Prolific Data').sum()\n",
    "    df_heading = df_heading.iloc[num : , :]\n",
    "\n",
    "    ranks = {col: {ques: rank for rank, ques in enumerate(df_heading[col].unique(),1)} for col in df_heading.columns}\n",
    "        \n",
    "    for category, rank in ranks.items():\n",
    "        df_heading[category + '_encoded'] = df_heading[category].map(rank)\n",
    "\n",
    "    df_heading['combined'] = df_heading[['header_first_encoded','header_second_encoded','header_third_encoded']].astype(str).apply(lambda x: '-'.join(x), axis=1)\n",
    "\n",
    "    return df_heading\n",
    "\n",
    "\n",
    "def extract_ques(df, code):\n",
    "\n",
    "    # returns all three levels of queries of the survey. where code is the header name in coded headers\n",
    "\n",
    "    first = df.loc[df['combined'] == code, 'header_first'].values[0]\n",
    "    sec = df.loc[df['combined'] == code, 'header_second'].values[0]\n",
    "    third = df.loc[df['combined'] == code, 'header_third'].values[0]\n",
    "\n",
    "    return first, sec, third\n",
    "\n",
    "\n",
    "def write_excel(df, dic, combinations_name):\n",
    "\n",
    "    # get data excel sheet\n",
    "    writer = pd.ExcelWriter(combinations_name, engine='xlsxwriter')\n",
    "    for iv, dv in dic[\"iv-dv\"].items():\n",
    "\n",
    "        # add ratings for IV\n",
    "        if iv in dic[\"iv\"][\"ordinal\"].keys():\n",
    "            ik = \"ordinal\"\n",
    "        else:\n",
    "            ik = \"nominal\"\n",
    "\n",
    "        for dvs in dv:\n",
    "\n",
    "            dvs = [col for col in df.columns if col.startswith(dvs)]\n",
    "\n",
    "            for one_dv in dvs:\n",
    "\n",
    "                df_temp = pd.DataFrame()\n",
    "                df_temp = pd.concat([df[iv], df[one_dv]], axis = 1)\n",
    "\n",
    "                # Drop all rows with values to drop in any column\n",
    "                df_temp = df_temp[~df_temp.isin(dic[\"values_to_drop\"]).any(axis=1)]\n",
    "\n",
    "                # add ratings for IV\n",
    "                df_temp['Rating IV'] = df_temp[iv].map(dic[\"iv\"][ik][iv])\n",
    "\n",
    "                # add ratings for DV\n",
    "                if one_dv[:4] in dic[\"dv\"][\"ordinal\"].keys():\n",
    "                    dk = \"ordinal\"\n",
    "                else:\n",
    "                    dk = \"nominal\"\n",
    "\n",
    "                df_temp['Rating DV'] = df_temp[one_dv].map(dic[\"dv\"][dk][one_dv[:4]])\n",
    "\n",
    "                sheet_name = str(iv) + ' - ' + str(one_dv)\n",
    "                df_temp.to_excel(writer, sheet_name = sheet_name, index = False)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def conti_table(var1, var2, xtick_list, ytick_list, margins = True,  plot = False):\n",
    "\n",
    "    contingency_table = pd.crosstab(var1, var2, margins = margins)\n",
    "\n",
    "    # code to display pretty\n",
    "    if margins == True:\n",
    "        xlab = xtick_list + ['total']\n",
    "        ylab = ytick_list + ['total']\n",
    "    else:\n",
    "        xlab = xtick_list\n",
    "        ylab = ytick_list\n",
    "\n",
    "    contingency_table.columns = xlab\n",
    "    contingency_table.index = ylab\n",
    "\n",
    "    # Display the contingency table\n",
    "    if plot == True:\n",
    "        print(tabulate(contingency_table , headers= xlab, tablefmt='grid'))\n",
    "\n",
    "    return contingency_table\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef09b86f",
   "metadata": {},
   "source": [
    "### Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11524d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetadata(df_temp, dic, df_heading, len_df): \n",
    "        \n",
    "    '''\n",
    "    df_temp -  is data frame \n",
    "    \n",
    "    Globals\n",
    "    dic - dictionary with all rankings\n",
    "    df_heading - list of custom headings for the project\n",
    "    df - orginal data frame\n",
    "    \n",
    "    Returns - a list of meta data '''\n",
    "\n",
    "    # EXTRACT METADATA\n",
    "    \n",
    "    # get all titles\n",
    "    IV_name = dic[df_temp.columns[0]]                                                   # IV\n",
    "\n",
    "    _, sec, third = extract_ques(df_heading, df_temp.columns[1])\n",
    "    if type(third) == str :\n",
    "            DV_name = third\n",
    "    else:\n",
    "            DV_name = sec\n",
    "    DV_name = DV_name[0] + re.sub(r'[A-Za-z]', \"\", DV_name[1:6])                        # DV\n",
    "\n",
    "    # get unique values to cal dof. dof = (r-1)(c-1)\n",
    "    IV_unique = len(df_temp[df_temp.columns[0]].unique())                               # Unique IV\n",
    "    DV_unique = len(df_temp[df_temp.columns[1]].unique())                               # Unique DV\n",
    "\n",
    "    # get data type of the IV and DV\n",
    "    IV_dataType = \"nominal\" if df_temp.columns[0] in list(dic[\"iv\"][\"nominal\"].keys()) else \"ordinal\"           # IV dataType\n",
    "    DV_dataType = \"nominal\" if df_temp.columns[1][:4] in list(dic[\"dv\"][\"nominal\"].keys()) else \"ordinal\"       # DV dataType\n",
    "\n",
    "    Num_rows = int(df_temp.shape[0])                                                    # Number of rows\n",
    "    Num_rows_dropped = int(len_df - Num_rows)                                           # Number of rows lost\n",
    "\n",
    "    return IV_name, DV_name, IV_unique ,DV_unique, IV_dataType, DV_dataType, Num_rows, Num_rows_dropped\n",
    "\n",
    "\n",
    "def suggest_statistical_tests(IV_dataType, DV_dataType, IV_unique, DV_unique):\n",
    "\n",
    "    # Statistical tests based on different combinations of variables and groups\n",
    "    # nominal_nominal_tests = [\"Chi-square test\", \"Fisher's exact test\"]\n",
    "    # nominal_ordinal_tests = ['Kruskal-Wallis test']\n",
    "    # ordinal_nominal_tests = ['Jonckheere-Terpstra test', 'Cochran-Armitage trend test']\n",
    "    # ordinal_ordinal_tests = ['Linear By Liner Test', 'Kendall correlation', 'Jonckheere-Terpstra test', \"Cuzick test\"]\n",
    "\n",
    "    # Check the characteristics of the variables and suggest appropriate tests\n",
    "    if IV_dataType == 'nominal' and DV_dataType == 'nominal' and IV_unique >= 2 and DV_unique >= 2:\n",
    "        return [\"Chi-square test\"]\n",
    "    \n",
    "\n",
    "    elif IV_dataType == 'nominal' and DV_dataType == 'ordinal' and IV_unique > 2 and DV_unique > 2:\n",
    "        return [\"Kruskal-Wallis test\"]\n",
    "    \n",
    "    \n",
    "    elif IV_dataType == 'ordinal' and DV_dataType == 'nominal' and IV_unique > 2 and DV_unique <= 2:\n",
    "        return [\"Cochran-Armitage trend test\"]\n",
    "\n",
    "\n",
    "    elif IV_dataType == 'ordinal' and DV_dataType == 'nominal' and IV_unique > 2 and DV_unique > 2:\n",
    "        return [\"No test due to increased complexity\"]\n",
    "    \n",
    "\n",
    "    elif IV_dataType == 'ordinal' and DV_dataType == 'ordinal' and IV_unique >= 2 and DV_unique >= 2:\n",
    "        return [\"Linear By Linear Test\", \"Kendall correlation\", \"Jonckheere-Terpstra test\", \"Cuzick test\"]\n",
    "    else:\n",
    "        return ['no test avaiable']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "719dfc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearsonsChi2Test(df_temp, IV_dataType, DV_dataType, significant_val):\n",
    "        \n",
    "    ''' PEARSON'S CHI2 TEST - for nominal DV and nominal IV\n",
    "    df_temp - your dataframe with one IV and one DV\n",
    "    IV_datatype - is it ordinal or nominal\n",
    "    DV_ datatype - is it ordinal or nominal\n",
    "    significant_val - alpha under the Null Hypothesis as a number'''\n",
    "\n",
    "    chi_contingency_table = pd.crosstab(df_temp[df_temp.columns[0]], df_temp[df_temp.columns[1]]) # do not use margins = True as it corrupts the result\n",
    "    observed = chi_contingency_table.values\n",
    "\n",
    "    # Calculate row and column totals\n",
    "    row_totals = observed.sum(axis=1)\n",
    "    column_totals = observed.sum(axis=0)\n",
    "\n",
    "    # Expected values\n",
    "    expected = np.outer(row_totals, column_totals)/chi_contingency_table.sum().sum()\n",
    "    chi_meet_condition = \"yes as expected values more than 5\" if (np.asarray(expected) < 5).sum() <= 0 else \"no as expected values less than 5\"       # Meets condition\n",
    "\n",
    "\n",
    "    #it accepts both Rated values and original string values - treats both their values as continuous \n",
    "    chi2, chi_pvalue, chi_dof = stats.contingency.chi2_contingency(chi_contingency_table, correction = False)[:3]                                     # Chi correlation, Chi P value, Chi dof\n",
    "    chi_significance = 'significant' if chi_pvalue < significant_val else 'not significant'\n",
    "\n",
    "\n",
    "    # Calculate Cramer V coefficient - calculate V when youâ€™re working with any table larger than a 2 x 2 contingency table\n",
    "    chi_effect_name = \"CramerV\"\n",
    "    cramer_dof = min(chi_contingency_table.shape) - 1\n",
    "    effect_size_cramer_V = np.sqrt(chi2 / (cramer_dof * chi_contingency_table.sum().sum()))                                                           # Cramer V coeff\n",
    "    cramerV_interpret = cramerV_interpretation(cramer_dof, effect_size_cramer_V)                                                                      # stat interpretation\n",
    "\n",
    "    return chi_meet_condition, chi2, chi_pvalue, chi_significance, chi_dof, chi_effect_name, effect_size_cramer_V, cramerV_interpret, chi_contingency_table\n",
    "\n",
    "\n",
    "def cramerV_interpretation(cramer_dof, number):\n",
    "\n",
    "    '''Interpret cramer V \n",
    "    cramer_dof - dof for cramer V min( row-1 , column -1 ) of contigency table\n",
    "    number - cramer V stat'''\n",
    "\n",
    "    cramer_table = { 1 : {'weak' : (0, 0.10) , 'moderate' : (0.10, 0.30), 'strong' : (0.30, 0.50), 'very strong': (0.50, 1)}, \n",
    "                    2 : {'weak' : (0, 0.07) , 'moderate' : (0.07, 0.21), 'strong' : (0.21, 0.35), 'very strong': (0.35, 1)},\n",
    "                    3 : {'weak' : (0, 0.06) , 'moderate' : (0.06, 0.17), 'strong' : (0.17, 0.29), 'very strong': (0.29, 1)}, \n",
    "                    4 : {'weak' : (0, 0.05) , 'moderate' : (0.05, 0.15), 'strong' : (0.15, 0.25), 'very strong': (0.25, 1)},\n",
    "                    5 : {'weak' : (0, 0.04) , 'moderate' : (0.04, 0.13), 'strong' : (0.13, 0.22), 'very strong': (0.22, 1)}}\n",
    "\n",
    "    if number <=0 :\n",
    "        return 'no assosiation'\n",
    "\n",
    "    for effect, range_tuple in cramer_table[cramer_dof].items():\n",
    "        lower_bound, upper_bound = range_tuple\n",
    "        if lower_bound <= number < upper_bound:\n",
    "            return effect\n",
    "\n",
    "    return 'value could not be found - check the cramerV table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff9a2613",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kruskalTest(df_temp, contingency_table, significant_val): \n",
    "        \n",
    "    '''KRUSKAL WALLIS - independent variable is treated as nominal\n",
    "    df_temp - your dataframe with one IV and one DV\n",
    "    contingency_table - contigency table computed for IV and DV without margins\n",
    "    IV_datatype - is it ordinal or nominal\n",
    "    DV_ datatype - is it ordinal or nominal\n",
    "    significant_val - alpha under the Null Hypothesis as a number'''\n",
    "\n",
    "    '''Null Hypothesis  H0=the independent Variable - age , income etc cause the similar response for the questions (all groups in IV originate from the same distribution and have the same median)\n",
    "    Alternative Hypothesis  HA=At least one of the groups in IV generates different distribution of response (at least one group originates from a different distribution and has a different median'''\n",
    "\n",
    "    test_name = \"Kruskal-Wallis test\"\n",
    "    assumption_met = \"populations from which the samples are drawn have similar underlying distributions\"\n",
    "\n",
    "    df_temp = df_temp.sort_values('Rating IV')\n",
    "    groups = [df_temp[df_temp['Rating IV'] == val]['Rating DV'] for val in df_temp['Rating IV'].unique()]\n",
    "    # the method with parsing groups is the correct method and gives correct outcomes\n",
    "    kruskal_stat, kruskal_pvalue = stats.kruskal(*groups)                                                                               # Kruskal stat, Kruskal P value\n",
    "    kruskal_significance = 'significant' if kruskal_pvalue < significant_val else 'not significant'                                     # Kruskal p value interpretation\n",
    "    kruskal_dof = len(df_temp['Rating IV'].unique()) - 1\n",
    "    \n",
    "    effect_name = \"epsilon squared\"\n",
    "    count = contingency_table.sum().sum()\n",
    "    kruskal_effectsize, effect_interpretation = epsilon2_interpretation(kruskal_stat, count)                                                                   # Kruskal effect size interpretation\n",
    "\n",
    "    return test_name, assumption_met, kruskal_stat, kruskal_pvalue, kruskal_significance, kruskal_dof, effect_name, kruskal_effectsize, effect_interpretation\n",
    "\n",
    "\n",
    "def epsilon2_interpretation(num, count):\n",
    "\n",
    "    '''Interpret Kriuskal Wallis \n",
    "    number - Kriuskal Wallis stat'''\n",
    "\n",
    "    num = abs(num)\n",
    "\n",
    "    number = (num * (count +1))/ (count**2 - 1)\n",
    "\n",
    "    epsilon_table = {'very weak' : (0, 0.01) , 'weak' : (0.01, 0.04), 'moderate' : (0.04, 0.16), 'relatively strong': (0.16, 0.36), 'strong':(0.36, 0.64), 'very strong': (0.64, 1.00) }\n",
    "\n",
    "    if number == 0 :\n",
    "        return number, 'no assosiation'\n",
    "\n",
    "    for effect, range_tuple in epsilon_table.items():\n",
    "        lower_bound, upper_bound = range_tuple\n",
    "        if lower_bound <= number < upper_bound:\n",
    "            return number, effect\n",
    "\n",
    "    return number, \"value could not be found - check the epsilon table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "692753ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cochranArmitageTest(df_temp, contingency_table, significant_val):\n",
    "\n",
    "    test_name = \"Cochran-Armitage trend test - two tailed\"\n",
    "    assumption_met = \"IV have natural ordering\"\n",
    "    result = sm.stats.Table(contingency_table).test_ordinal_association()\n",
    "    significance = 'significant' if result.pvalue < significant_val else 'not significant' \n",
    "    doff =  len(df_temp['Rating IV'].unique()) - 1\n",
    "\n",
    "    effect_name = \"Epsilon squared\"\n",
    "    count = contingency_table.sum().sum()\n",
    "    effect_size, effect_interpretation = epsilon2_interpretation(result.statistic, count)     \n",
    "    \n",
    "    return test_name, assumption_met, result.statistic, result.pvalue, significance, doff, effect_name, effect_size, effect_interpretation\n",
    "\n",
    "\n",
    "def ordinalChi2Test(df_temp, contingency_table, significant_val):\n",
    "\n",
    "    '''LINERA TO LINEAR TEST (ORDINAL CHI SQUARE TEST) + KENDALL TAU\n",
    "    df_temp - your dataframe with one IV and one DV\n",
    "    contingency_table - contigency table computed for IV and DV without margins\n",
    "    IV_unique, DV_unique - uniques categories in IV and DV\n",
    "    chi2 - as computed in pearsonsChi2Test function\n",
    "    chi_dof - as computed in pearsonsChi2Test function\n",
    "    significant_val - alpha under the Null Hypothesis as a number'''\n",
    "\n",
    "\n",
    "    # M2 = chi-square statistic on 1 degree of freedom based on pearson's r . The reasource used pearson's\n",
    "    test_name = \"linear 2 linear\"\n",
    "    assumption_met = \"Both IV and DV are ordinal\"\n",
    "    df = 1\n",
    "    o_chi_pearsonsr_stat, _ = stats.pearsonr(df_temp['Rating IV'], df_temp['Rating DV']) # use ranks and not the categorical values                                                   \n",
    "    test_statistic = (contingency_table.sum().sum() - 1) * o_chi_pearsonsr_stat**2                                                                                                # Ochi stat (pearsons) dof = 1\n",
    "    o_chi_p_pvalue = 1 - stats.chi2.cdf(test_statistic, df = df)                                                                                                                  # Ochi P value (pearson's)\n",
    "    o_chi_p_significance = 'significant' if o_chi_p_pvalue < significant_val else 'not significant'                                                                     # Ochi P val intrepretation (pearsons)\n",
    "    effect_name = pd.NA\n",
    "    effect_size, effect_interpretation = pd.NA, pd.NA\n",
    "\n",
    "    # m2_k = (contingency_table.sum().sum() - 1) * o_chi_kendalltau_stat**2                                                                                               # Ochi stat (kendall) dof = 1\n",
    "    # o_chi_k_pvalue = 1 - stats.chi2.cdf(m2_k, df = df)                                                                                                                  # Ochi P value (kendall)\n",
    "    # o_chi_k_significance = 'significant' if o_chi_k_pvalue < significant_val else 'not significant'                                                                     # Ochi P val interpretation (kendall)\n",
    "\n",
    "    return test_name, assumption_met, test_statistic, o_chi_p_pvalue, o_chi_p_significance, df, effect_name, effect_size, effect_interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "021629f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kendallTauTest(df_temp, IV_unique, DV_unique, significant_val):\n",
    "\n",
    "    test_name = \"Kendall Tau\"\n",
    "    assumption_met = \"Ordinal to ordinal data\"\n",
    "\n",
    "    # M2 = chi-square statistic on 1 degree of freedom based on kendalltau r \n",
    "    variant = 'b' if IV_unique == DV_unique else 'c'\n",
    "    o_chi_kendalltau_stat, kt_pvalue = stats.kendalltau(df_temp['Rating IV'], df_temp['Rating DV'], variant = variant) # use ranks and not the categorical values       # Kendall tau stat, Kendall tau P value\n",
    "    kendalltau_significance = 'significant' if kt_pvalue < significant_val else 'not significant'                                                                       # Kendall tau significance\n",
    "    doff = (IV_unique - 1)*(DV_unique-1)\n",
    "    effect_name = \"tau\"\n",
    "    kendalltau_effectsize = o_chi_kendalltau_stat                                                                                                                       # Kendall tau Effect size\n",
    "    effect_interpretation = kendalltau_interpretation(o_chi_kendalltau_stat)    \n",
    "   \n",
    "    return test_name, assumption_met, o_chi_kendalltau_stat, kt_pvalue, kendalltau_significance, doff, effect_name, kendalltau_effectsize, effect_interpretation\n",
    "\n",
    "\n",
    "def kendalltau_interpretation(number):\n",
    "\n",
    "    '''Interpret Kendal tau \n",
    "    number - Kendal tau stat'''\n",
    "\n",
    "    number = abs(number)\n",
    "\n",
    "    kendall_table = {'very weak' : (0, 0.10) , 'weak' : (0.10, 0.20), 'moderate' : (0.20, 0.30), 'very strong': (0.30, 1)}\n",
    "\n",
    "    if number == 0 :\n",
    "        return 'no assosiation'\n",
    "\n",
    "    for effect, range_tuple in kendall_table.items():\n",
    "        lower_bound, upper_bound = range_tuple\n",
    "        if lower_bound <= number < upper_bound:\n",
    "            return effect\n",
    "\n",
    "    return 'value could not be found - check the kendall table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a69e87b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ONStatisticTests(data_workbook, dic, df_heading):\n",
    "\n",
    "    # Data is excel file with sheets containing combination of IV and DV and their scores\n",
    "    \n",
    "    df_result = pd.DataFrame(columns=[\"IV\", \"DV\", \"Unique IV\", \"Unique DV\", \"IV dataType\", \"DV dataType\", \"Number of rows\", \"Number of rows lost\", \"suggested tests\", \\\n",
    "        \"test_name\", \"assumption_met\", \"test_statistic\", \"p_value\", \"significance\", \"doff\", \"effect_name\", \"effect_size\", \"effect_interpretation\", \\\n",
    "            \"Chi2Test\", \"chi_assumption_met\", \"chi_stat\", \"chi_p_value\", \"chi_significance\", \"chi_doff\", \"chi_effect_name\", \"chi_effect_size\", \"chi_effect_interpretation\"]) #, \"chi_deviation\", \"chi_deviation_dof\", \"deviation_pvalue\", \"deviation_significance\"]) \n",
    "                                    \n",
    "    significant_val = 0.05\n",
    "    len_df = 499\n",
    "\n",
    "    # load the sheet\n",
    "    excel = pd.ExcelFile(data_workbook)\n",
    "\n",
    "    # run test on each sheet \n",
    "    for sheet_name in excel.sheet_names: \n",
    "\n",
    "        df_temp = pd.read_excel(data_workbook, sheet_name=sheet_name)\n",
    "\n",
    "        # META BLOCK\n",
    "        IV_name, DV_name, IV_unique ,DV_unique, IV_dataType, DV_dataType, Num_rows, Num_rows_dropped = getMetadata(df_temp, dic, df_heading, len_df)\n",
    "\n",
    "        tests = suggest_statistical_tests(IV_dataType, DV_dataType, IV_unique, DV_unique)\n",
    "\n",
    "        # chi test everytime\n",
    "        chi_assumption_met, chi_stat, chi_p_value, chi_significance, chi_doff, chi_effect_name, chi_effect_size, \\\n",
    "                    chi_effect_interpretation, contingency_table = pearsonsChi2Test(df_temp, IV_dataType, DV_dataType, significant_val)\n",
    "        \n",
    "        # chi_deviation, chi_deviation_dof, deviation_pvalue, \\\n",
    "        #                 deviation_significance = \n",
    "\n",
    "        for test in tests:\n",
    "\n",
    "            print(test)\n",
    "            \n",
    "            if test == \"Kruskal-Wallis test\":\n",
    "                test_name, assumption_met, test_statistic, p_value, significance, doff, effect_name, effect_size, effect_interpretation = kruskalTest(df_temp, contingency_table, significant_val)\n",
    "\n",
    "                \n",
    "            elif test == \"Cochran-Armitage trend test\":\n",
    "                test_name, assumption_met, test_statistic, p_value, significance, doff, effect_name, effect_size, effect_interpretation = cochranArmitageTest(df_temp, contingency_table, significant_val)\n",
    "                \n",
    "\n",
    "            elif test == \"Linear By Linear Test\":\n",
    "                test_name, assumption_met, test_statistic, p_value, significance, doff, effect_name, effect_size, effect_interpretation = ordinalChi2Test(df_temp, contingency_table, significant_val)\n",
    "                \n",
    "\n",
    "            elif test == \"Kendall correlation\": \n",
    "                test_name, assumption_met, test_statistic, p_value, significance, doff, effect_name, effect_size, effect_interpretation = kendallTauTest(df_temp, IV_unique, DV_unique, significant_val)\n",
    "                \n",
    "            else:\n",
    "                test_name = \"No dedicated test carried out due to increased complexity. check chi results or R outputs\"\n",
    "                assumption_met, test_statistic, p_value, significance, doff, effect_name, effect_size, effect_interpretation = pd.NA, pd.NA, pd.NA, pd.NA, pd.NA, pd.NA, pd.NA, pd.NA\n",
    "\n",
    "            \n",
    "            lst = [IV_name, DV_name, IV_unique ,DV_unique, IV_dataType, DV_dataType, Num_rows, Num_rows_dropped, tests,\\\n",
    "                   test_name, assumption_met, test_statistic, p_value, significance, doff, effect_name, effect_size, effect_interpretation, \\\n",
    "                   \"Chi2Test\", chi_assumption_met, chi_stat, chi_p_value, chi_significance, chi_doff, chi_effect_name, chi_effect_size, chi_effect_interpretation] #, chi_deviation, chi_deviation_dof, deviation_pvalue, deviation_significance]\n",
    "            \n",
    "            df_result.loc[len(df_result)] = lst\n",
    "\n",
    "    return df_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statistics-project-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
