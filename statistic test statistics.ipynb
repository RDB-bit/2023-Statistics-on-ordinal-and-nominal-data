{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d35522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import json\n",
    "# import xlsxwriter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "452a8248",
   "metadata": {},
   "source": [
    "### Non Parametric Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49639559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ordinal data sheet (remember one IV = Employment status is nominal and rest are ordinal, All Dvs are ordinal)\n",
    "\n",
    "workbooks = ['SampleDataOrdinal.xlsx', 'SampleDataNominal.xlsx']\n",
    "\n",
    "for data_workbook in workbooks:\n",
    "\n",
    "    df_result = ONStatisticTests(data_workbook)\n",
    "    # change independent_data_type , dependent_data_type in the META BLOCK or add code to decipher the variable value\n",
    "\n",
    "    df_result.to_excel('results' + data_workbook[:-5] + '.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8102592",
   "metadata": {},
   "source": [
    "### Helper Generic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2399d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetadata(df_temp, independent_data_type = 'ordinal', dependent_data_type = 'ordinal'):\n",
    "        \n",
    "    '''\n",
    "    df_temp -  is data frame \n",
    "    independent_data_type - is the independent data ordinal or nominal. Default = ordinal\n",
    "    dependent_data_type - is the dependent data ordinal or nominal. Default = ordinal\n",
    "    \n",
    "    Returns - a list of meta data '''\n",
    "\n",
    "    # EXTRACT METADATA\n",
    "\n",
    "    # get unique values to cal dof. dof = (r-1)(c-1)\n",
    "    IV_unique = len(df_temp[df_temp.columns[0]].unique())                               # Unique IV\n",
    "    DV_unique = len(df_temp[df_temp.columns[1]].unique())                               # Unique DV\n",
    "\n",
    "    # get data type of the IV and DV\n",
    "    IV_dataType = independent_data_type                                                 # IV dataType\n",
    "    DV_dataType = dependent_data_type                                                   # DV dataType\n",
    "\n",
    "    # if you drop some rows as they dont have data that makes sense in an order , like 'don't know' and 'prefer not to say'\n",
    "    # Num_rows = int(df_temp.shape[0])                                                  # Number of rows\n",
    "    # Num_rows_dropped = int(originalRowNum - Num_rows)                                 # Number of rows lost\n",
    "\n",
    "    return IV_unique ,DV_unique, IV_dataType, DV_dataType\n",
    "\n",
    "\n",
    "\n",
    "def cramerV_interpretation(cramer_dof, number):\n",
    "\n",
    "    '''Interpret cramer V \n",
    "    cramer_dof - dof for cramer V min( row-1 , column -1 ) of contigency table\n",
    "    number - cramer V stat'''\n",
    "\n",
    "    cramer_table = { 1 : {'weak' : (0, 0.10) , 'moderate' : (0.10, 0.30), 'strong' : (0.30, 0.50), 'very strong': (0.50, 1)}, \n",
    "                    2 : {'weak' : (0, 0.07) , 'moderate' : (0.07, 0.21), 'strong' : (0.21, 0.35), 'very strong': (0.35, 1)},\n",
    "                    3 : {'weak' : (0, 0.06) , 'moderate' : (0.06, 0.17), 'strong' : (0.17, 0.29), 'very strong': (0.29, 1)}, \n",
    "                    4 : {'weak' : (0, 0.05) , 'moderate' : (0.05, 0.15), 'strong' : (0.15, 0.25), 'very strong': (0.25, 1)},\n",
    "                    5 : {'weak' : (0, 0.04) , 'moderate' : (0.04, 0.13), 'strong' : (0.13, 0.22), 'very strong': (0.22, 1)}}\n",
    "\n",
    "    if number <=0 :\n",
    "        return 'no assosiation'\n",
    "\n",
    "    for effect, range_tuple in cramer_table[cramer_dof].items():\n",
    "        lower_bound, upper_bound = range_tuple\n",
    "        if lower_bound <= number < upper_bound:\n",
    "            return effect\n",
    "\n",
    "    return 'value could not be found - check the cramerV table'\n",
    "\n",
    "\n",
    "def pearsonsChi2Test(df_temp, IV_dataType, DV_dataType, significant_val):\n",
    "        \n",
    "    ''' PEARSON'S CHI2 TEST - for nominal DV and nominal IV\n",
    "    df_temp - your dataframe with one IV and one DV\n",
    "    IV_datatype - is it ordinal or nominal\n",
    "    DV_ datatype - is it ordinal or nominal\n",
    "    significant_val - alpha under the Null Hypothesis as a number'''\n",
    "\n",
    "    # prepare data\n",
    "    chi_validity = \"valid as DV is nominal and IV is nominal\" if DV_dataType == \"nominal\" and IV_dataType == \"nominal\" else \"not valid as either DV or IV is ordinal\"        # Chi2 validity\n",
    "\n",
    "    chi_contingency_table = pd.crosstab(df_temp[df_temp.columns[0]], df_temp[df_temp.columns[1]]) # do not use margins = True as it corrupts the result\n",
    "    observed = chi_contingency_table.values\n",
    "\n",
    "    # Calculate row and column totals\n",
    "    row_totals = observed.sum(axis=1)\n",
    "    column_totals = observed.sum(axis=0)\n",
    "\n",
    "    # Expected values\n",
    "    expected = np.outer(row_totals, column_totals)/chi_contingency_table.sum().sum()\n",
    "    chi_meet_condition = \"yes as expected values more than 5\" if (np.asarray(expected) < 5).sum() <= 0 else \"no as expected values less than 5\"       # Meets condition\n",
    "\n",
    "\n",
    "    #it accepts both Rated values and original string values - treats both their values as continuous \n",
    "    chi2, chi_pvalue, chi_dof = stats.contingency.chi2_contingency(chi_contingency_table, correction = False)[:3]                                     # Chi correlation, Chi P value, Chi dof\n",
    "    chi_significance = 'significant' if chi_pvalue < significant_val else 'not significant'\n",
    "\n",
    "\n",
    "    # Calculate Cramer V coefficient - calculate V when youâ€™re working with any table larger than a 2 x 2 contingency table\n",
    "    cramer_dof = min(chi_contingency_table.shape) - 1\n",
    "    effect_size_cramer_V = np.sqrt(chi2 / (cramer_dof * chi_contingency_table.sum().sum()))                                                           # Cramer V coeff\n",
    "    cramerV_interpret = cramerV_interpretation(cramer_dof, effect_size_cramer_V)                                                                      # stat interpretation\n",
    "\n",
    "    return chi_validity, chi_meet_condition, chi2, chi_pvalue, chi_significance, chi_dof, effect_size_cramer_V, cramerV_interpret, chi_contingency_table\n",
    "\n",
    "\n",
    "\n",
    "def ordinalChi2Test(df_temp, contingency_table, DV_dataType, IV_dataType, IV_unique, DV_unique, chi2, chi_dof, significant_val):\n",
    "\n",
    "    '''LINERA TO LINEAR TEST (ORDINAL CHI SQUARE TEST) + KENDALL TAU\n",
    "    df_temp - your dataframe with one IV and one DV\n",
    "    contingency_table - contigency table computed for IV and DV without margins\n",
    "    IV_datatype - is it ordinal or nominal\n",
    "    DV_ datatype - is it ordinal or nominal\n",
    "    IV_unique, DV_unique - uniques categories in IV and DV\n",
    "    chi2 - as computed in pearsonsChi2Test function\n",
    "    chi_dof - as computed in pearsonsChi2Test function\n",
    "    significant_val - alpha under the Null Hypothesis as a number'''\n",
    "\n",
    "    # prepare data\n",
    "    o_chi_validity = \"valid as DV and IV are ordinal\" if (DV_dataType == \"ordinal\" and IV_dataType == \"ordinal\") else \"invalid as either DV or IV is nominal\"             # Linear 2 Linear validity\n",
    "\n",
    "    if o_chi_validity == \"invalid as either DV or IV is nominal\" :\n",
    "\n",
    "        print(' returning nans')\n",
    "\n",
    "        return [np.nan] * 16\n",
    "\n",
    "    else:\n",
    "\n",
    "        # M2 = chi-square statistic on 1 degree of freedom based on pearson's r . The reasource used pearson's\n",
    "        df = 1\n",
    "        o_chi_pearsonsr_stat, _ = stats.pearsonr(df_temp['Rating IV'], df_temp['Rating DV']) # use ranks and not the categorical values                                                   \n",
    "        m2_p = (contingency_table.sum().sum() - 1) * o_chi_pearsonsr_stat**2                                                                                                # Ochi stat (pearsons) dof = 1\n",
    "        o_chi_p_pvalue = 1 - stats.chi2.cdf(m2_p, df = df)                                                                                                                  # Ochi P value (pearson's)\n",
    "        o_chi_p_significance = 'significant' if o_chi_p_pvalue < significant_val else 'not significant'                                                                     # Ochi P val intrepretation (pearsons)\n",
    "\n",
    "\n",
    "        # M2 = chi-square statistic on 1 degree of freedom based on kendalltau r \n",
    "        kendalltau_validity = \"valid as DV and IV are ordinal\" if DV_dataType == \"ordinal\" and IV_dataType == \"ordinal\" else \"invalid as either DV or IV is nominal\"        # Kendall tau validity\n",
    "        variant = 'b' if IV_unique == DV_unique else 'c'\n",
    "        o_chi_kendalltau_stat, kt_pvalue = stats.kendalltau(df_temp['Rating IV'], df_temp['Rating DV'], variant = variant) # use ranks and not the categorical values       # Kendall tau stat, Kendall tau P value\n",
    "        kendalltau_significance = 'significant' if kt_pvalue < significant_val else 'not significant'                                                                       # Kendall tau significance\n",
    "        kendalltau_effectsize = kendalltau_interpretation(o_chi_kendalltau_stat)                                                                                            # Kendall tau Effect size\n",
    "\n",
    "        m2_k = (contingency_table.sum().sum() - 1) * o_chi_kendalltau_stat**2                                                                                               # Ochi stat (kendall) dof = 1\n",
    "        o_chi_k_pvalue = 1 - stats.chi2.cdf(m2_k, df = df)                                                                                                                  # Ochi P value (kendall)\n",
    "        o_chi_k_significance = 'significant' if o_chi_k_pvalue < significant_val else 'not significant'                                                                     # Ochi P val interpretation (kendall)\n",
    "\n",
    "        # Deviation from linear\n",
    "        chi_deviation_from_linear = abs(m2_p - chi2)                                                                                                                        # Deviation from linear\n",
    "        chi_deviation_dof = chi_dof - 1                                                                                                                                     # Deviation dof\n",
    "        deviation_pvalue = stats.chi2.cdf(chi_deviation_from_linear, df = chi_deviation_dof)                                                                                # Deviation P value\n",
    "        deviation_significance = 'significant' if deviation_pvalue < significant_val else 'not significant'                                                                 # Deviation P val interpretation\n",
    "\n",
    "        return o_chi_validity, m2_p, o_chi_p_pvalue, o_chi_p_significance, m2_k, o_chi_k_pvalue, o_chi_k_significance, \\\n",
    "            chi_deviation_from_linear, chi_deviation_dof, deviation_pvalue, deviation_significance, \\\n",
    "                kendalltau_validity, o_chi_kendalltau_stat, kendalltau_effectsize, kt_pvalue, kendalltau_significance\n",
    "\n",
    "\n",
    "def kendalltau_interpretation(number):\n",
    "\n",
    "    '''Interpret Kendal tau \n",
    "    number - Kendal tau stat'''\n",
    "\n",
    "    number = abs(number)\n",
    "\n",
    "    kendall_table = {'very weak' : (0, 0.10) , 'weak' : (0.10, 0.20), 'moderate' : (0.20, 0.30), 'very strong': (0.30, 1)}\n",
    "\n",
    "    if number == 0 :\n",
    "        return 'no assosiation'\n",
    "\n",
    "    for effect, range_tuple in kendall_table.items():\n",
    "        lower_bound, upper_bound = range_tuple\n",
    "        if lower_bound <= number < upper_bound:\n",
    "            return effect\n",
    "\n",
    "    return 'value could not be found - check the kendall table'\n",
    "\n",
    "\n",
    "\n",
    "def kruskalTest(df_temp, IV_dataType, DV_dataType, contingency_table, significant_val): \n",
    "        \n",
    "    '''KRUSKAL WALLIS - independent variable is treated as nominal\n",
    "    df_temp - your dataframe with one IV and one DV\n",
    "    contingency_table - contigency table computed for IV and DV without margins\n",
    "    IV_datatype - is it ordinal or nominal\n",
    "    DV_ datatype - is it ordinal or nominal\n",
    "    significant_val - alpha under the Null Hypothesis as a number'''\n",
    "\n",
    "    kruskal_validity = \"valid as DV is ordinal\" if DV_dataType == \"ordinal\" else \"not valid as DV is nominal\"  # validity\n",
    "\n",
    "    if kruskal_validity == \"not valid as DV is nominal\":\n",
    "\n",
    "        print(' returning nans')\n",
    "\n",
    "        return [np.nan] * 5\n",
    "\n",
    "    else:\n",
    "\n",
    "        groups = [df_temp[df_temp['Rating IV'] == val]['Rating DV'] for val in df_temp['Rating IV'].unique()]\n",
    "        kruskal_stat, kruskal_pvalue = stats.kruskal(*groups)                                                                               # Kruskal stat, Kruskal P value\n",
    "        kruskal_significance = 'significant' if kruskal_pvalue < significant_val else 'not significant'                                     # Kruskal p value interpretation\n",
    "\n",
    "        count = contingency_table.sum().sum()\n",
    "        kruskal_effectsize = epsilon2_interpretation(kruskal_stat, count)                                                                   # Kruskal effect size interpretation\n",
    "\n",
    "        return kruskal_validity, kruskal_stat, kruskal_pvalue, kruskal_significance, kruskal_effectsize\n",
    "\n",
    "\n",
    "\n",
    "def epsilon2_interpretation(num, count):\n",
    "\n",
    "    '''Interpret Kriuskal Wallis \n",
    "    number - Kriuskal Wallis stat'''\n",
    "\n",
    "    num = abs(num)\n",
    "\n",
    "    number = (num * (count +1))/ (count**2 - 1)\n",
    "\n",
    "    epsilon_table = {'very weak' : (0, 0.01) , 'weak' : (0.01, 0.04), 'moderate' : (0.04, 0.16), 'relatively strong': (0.16, 0.36), 'strong':(0.36, 0.64), 'very strong': (0.64, 1.00) }\n",
    "\n",
    "    if number == 0 :\n",
    "        return 'no assosiation'\n",
    "\n",
    "    for effect, range_tuple in epsilon_table.items():\n",
    "        lower_bound, upper_bound = range_tuple\n",
    "        if lower_bound <= number < upper_bound:\n",
    "            return effect\n",
    "\n",
    "    return 'value could not be found - check the epsilon table'\n",
    "\n",
    "\n",
    "def ONStatisticTests(data_workbook):\n",
    "\n",
    "    # Data is excel file with sheets containing combination of IV and DV and their scores\n",
    "\n",
    "    df_result = pd.DataFrame(columns=[\"Unique IV\", \"Unique DV\", \"IV dataType\", \"DV dataType\",  \\\n",
    "                            \"Chi2 validity\", \"Meets condition\", 'Chi stat', 'Chi P value', \"Chi significance\", \"Chi dof\", \"Effect size Cramer V coeff\", \"Chi2 Effect size interpretation\",\\\n",
    "                            \"Linear 2 Linear validity\", \"Ochi stat (pearsons) dof = 1\",  \"Ochi P value (pearson's)\", \"Ochi P value (pearson's) Significance\", \\\n",
    "                                    \"Ochi stat (kendall) dof = 1\", \"Ochi P value (kendall)\", \"Ochi P value (kendall's) Significance\", \\\n",
    "                                            \"Deviation from linear\", \"Deviation dof\", \"Deviation P value\", \"Deviation P value Significance\", \\\n",
    "                                                    \"Kendall tau validity\", \"Kendall tau stat\", \"Kendall Effect size interpretation\", \"Kendall tau P value\", \"Kendall tau P value Significance\", \\\n",
    "                                                            \"Kruskal validity\", \"Kruskal stat\", \"Kruskal P value\", \"Kruskal significance\", \"Kruskal Effectsize interpretation\"])\n",
    "\n",
    "    significant_val = 0.05\n",
    "\n",
    "    # load the sheet\n",
    "    excel = pd.ExcelFile(data_workbook)\n",
    "\n",
    "    # run test on each sheet \n",
    "    for sheet_name in excel.sheet_names: \n",
    "\n",
    "        df_temp = pd.read_excel(data_workbook, sheet_name=sheet_name)\n",
    "\n",
    "        # META BLOCK\n",
    "        IV_unique ,DV_unique, IV_dataType, DV_dataType = getMetadata(df_temp, independent_data_type = 'ordinal', dependent_data_type = 'ordinal')\n",
    "\n",
    "        # CHI2 BLOCK - NN\n",
    "        chi_validity, chi_meet_condition, chi2, chi_pvalue, chi_significance, chi_dof, effect_size_cramer_V, cramerV_interpret, \\\n",
    "                chi_contingency_table = pearsonsChi2Test(df_temp, IV_dataType, DV_dataType, significant_val)\n",
    "\n",
    "        # L2L and KENDALL TAU BLOCK -OO\n",
    "        o_chi_validity, m2_p, o_chi_p_pvalue, o_chi_p_significance, m2_k, o_chi_k_pvalue, o_chi_k_significance, chi_deviation_from_linear, chi_deviation_dof, \\\n",
    "                deviation_pvalue, deviation_significance, kendalltau_validity, o_chi_kendalltau_stat, kendalltau_effectsize, kt_pvalue, \\\n",
    "                        kendalltau_significance = ordinalChi2Test(df_temp, chi_contingency_table, DV_dataType, IV_dataType, IV_unique, DV_unique, chi2, chi_dof, significant_val)\n",
    "\n",
    "        # KRUSKAL WALLIS BLOCK - NO\n",
    "        kruskal_validity, kruskal_stat, kruskal_pvalue, kruskal_significance, kruskal_effectsize = kruskalTest(df_temp, IV_dataType, DV_dataType, chi_contingency_table, significant_val)\n",
    "\n",
    "\n",
    "        lst = [IV_unique ,DV_unique, IV_dataType, DV_dataType, chi_validity, chi_meet_condition, chi2, chi_pvalue, chi_significance, chi_dof, effect_size_cramer_V, cramerV_interpret, \\\n",
    "        o_chi_validity, m2_p, o_chi_p_pvalue, o_chi_p_significance, m2_k, o_chi_k_pvalue, o_chi_k_significance, chi_deviation_from_linear, chi_deviation_dof, \\\n",
    "                deviation_pvalue, deviation_significance, kendalltau_validity, o_chi_kendalltau_stat, kendalltau_effectsize, kt_pvalue, \\\n",
    "                        kendalltau_significance, kruskal_validity, kruskal_stat, kruskal_pvalue, kruskal_significance, kruskal_effectsize]\n",
    "        \n",
    "        df_result.loc[len(df_result)] = lst\n",
    "\n",
    "    return df_result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statistics-project-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
